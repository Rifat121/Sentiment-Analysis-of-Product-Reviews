{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('AMAZON MODIFIED.csv', encoding='unicode_escape')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews.numHelpful</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>sarcastic(y/n)</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>admiration</th>\n",
       "      <th>angry</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>FREAKIN' AWESOME</td>\n",
       "      <td>Amazon - Echo Show is the best of all my Amazo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful tablet at a price that ca not be beat</td>\n",
       "      <td>This tablet is a steal for the price. bought t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Must have</td>\n",
       "      <td>Everyone should have this product. It is great...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Great tablet.</td>\n",
       "      <td>Great tablet. Drop it many times and its still...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>A Great Buy</td>\n",
       "      <td>I am normally a real book person, but I have a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviews.numHelpful  reviews.rating  \\\n",
       "0                 0.0               5   \n",
       "1                 0.0               5   \n",
       "2                 0.0               5   \n",
       "3                 0.0               5   \n",
       "4                 3.0               5   \n",
       "\n",
       "                                     reviews.title  \\\n",
       "0                                 FREAKIN' AWESOME   \n",
       "1  Wonderful tablet at a price that ca not be beat   \n",
       "2                                        Must have   \n",
       "3                                    Great tablet.   \n",
       "4                                      A Great Buy   \n",
       "\n",
       "                                        reviews.text  sarcastic(y/n)  \\\n",
       "0  Amazon - Echo Show is the best of all my Amazo...             1.0   \n",
       "1  This tablet is a steal for the price. bought t...             1.0   \n",
       "2  Everyone should have this product. It is great...             1.0   \n",
       "3  Great tablet. Drop it many times and its still...             1.0   \n",
       "4  I am normally a real book person, but I have a...             1.0   \n",
       "\n",
       "   positive  negative  neutral  admiration  angry  Unnamed: 10  \n",
       "0       1.0       0.0      0.0         1.0    0.0          2.0  \n",
       "1       1.0       0.0      0.0         0.0    0.0          3.0  \n",
       "2       1.0       0.0      0.0         0.0    0.0          3.0  \n",
       "3       1.0       0.0      0.0         0.0    0.0          3.0  \n",
       "4       1.0       0.0      0.0         0.0    0.0          3.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    2005\n",
      "4    1208\n",
      "1     966\n",
      "3     181\n",
      "2      66\n",
      "Name: reviews.rating, dtype: int64\n",
      "0.0    785\n",
      "1.0     93\n",
      "Name: sarcastic(y/n), dtype: int64\n",
      "1.0    2420\n",
      "0.0    2004\n",
      "Name: positive, dtype: int64\n",
      "0.0    3352\n",
      "1.0    1068\n",
      "Name: negative, dtype: int64\n",
      "0.0    3478\n",
      "1.0     943\n",
      "Name: neutral, dtype: int64\n",
      "0.0    3497\n",
      "1.0     926\n",
      "Name: admiration, dtype: int64\n",
      "0.0    3667\n",
      "1.0     757\n",
      "Name: angry, dtype: int64\n",
      "3.0    1479\n",
      "4.0     924\n",
      "2.0     894\n",
      "7.0     683\n",
      "5.0     272\n",
      "Name: Unnamed: 10, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df.columns:\n",
    "    if(cl=='reviews.text' or cl=='reviews.title' or cl=='reviews.numHelpful'):\n",
    "        continue\n",
    "    print(df[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>illud</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>oddly</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30361</th>\n",
       "      <td>roulotte</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21468</th>\n",
       "      <td>maternity</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27515</th>\n",
       "      <td>prematurely</td>\n",
       "      <td>#fear</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16009</th>\n",
       "      <td>herculean</td>\n",
       "      <td>#fear</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28580</th>\n",
       "      <td>quietist</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14963</th>\n",
       "      <td>granulate</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12967</th>\n",
       "      <td>fiberscope</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6934</th>\n",
       "      <td>confraternity</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word    Primary    Secondary Polarity\n",
       "17088          illud  #interest    #interest    0.079\n",
       "24331          oddly     #anger     #disgust    -0.83\n",
       "30361       roulotte       #joy  #admiration    0.023\n",
       "21468      maternity  #interest  #admiration    0.088\n",
       "27515    prematurely      #fear     #disgust    -0.81\n",
       "16009      herculean      #fear     #disgust    -0.97\n",
       "28580       quietist  #interest  #admiration    0.081\n",
       "14963      granulate  #interest  #admiration    0.027\n",
       "12967     fiberscope       #joy  #admiration    0.173\n",
       "6934   confraternity       #joy    #surprise    0.863"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[0] = 1\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[1] = 0\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[4] = 2\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[2] = 4\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[5] = 7\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[7] = 5\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[6] = 3\n",
    "opposite[\"#disgust\"] = \"#interest\"\n",
    "opposite[3] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "def normal_algo(sen):\n",
    "    NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "    NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "    CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(t[0] in punct or t in CONJUCTION_WORDS):\n",
    "            nflag=False\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "            negatives.append(t)\n",
    "        if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "            nflag=True\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        \"\"\"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "        \"\"\"\n",
    "    #print(mark_neg)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "\n",
    "# tag1 = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n",
      "4426\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['reviews.text']]\n",
    "negatives = []\n",
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['reviews.text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4394\n",
      "1      32\n",
      "dtype: int64\n",
      "0    3672\n",
      "1     754\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(qmark).value_counts())\n",
    "print(pd.Series(exmark).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>sarcastic(y/n)</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>admiration</th>\n",
       "      <th>angry</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>FREAKIN' AWESOME</td>\n",
       "      <td>Amazon - Echo Show is the best of all my Amazo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazon echo show be best amazon product i love...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful tablet at a price that ca not be beat</td>\n",
       "      <td>This tablet is a steal for the price. bought t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tablet be steal price bought one sister read b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Must have</td>\n",
       "      <td>Everyone should have this product. It is great...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>everyone have product be great have be need se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great tablet.</td>\n",
       "      <td>Great tablet. Drop it many times and its still...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>great tablet drop many time still work</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A Great Buy</td>\n",
       "      <td>I am normally a real book person, but I have a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i be normally real book person i have year old...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous Gifting item.</td>\n",
       "      <td>I bought this as a gift to my dad and he loves...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i bought gift dad love well dont plan reading ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Child loves her Kindle</td>\n",
       "      <td>I bought this for my daughter. It is our 4th K...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i bought daughter be th kindle bought family t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Alternative to Ipad</td>\n",
       "      <td>I bought this for my kids its a cheap alternat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i bought kid cheap alternative ipad mini lose ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>I bought this with paper white and did not see...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i bought paper white do not see much differenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>I do not know why I waited so long</td>\n",
       "      <td>I find myself saying to my phone, \"Alexa, play...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i find say phone alexa play i wish make car too</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviews.rating                                    reviews.title  \\\n",
       "0               5                                 FREAKIN' AWESOME   \n",
       "1               5  Wonderful tablet at a price that ca not be beat   \n",
       "2               5                                        Must have   \n",
       "3               5                                    Great tablet.   \n",
       "4               5                                      A Great Buy   \n",
       "5               5                           Fabulous Gifting item.   \n",
       "6               5                           Child loves her Kindle   \n",
       "7               5                        Great Alternative to Ipad   \n",
       "8               5                                        Excellent   \n",
       "9               5               I do not know why I waited so long   \n",
       "\n",
       "                                        reviews.text  sarcastic(y/n)  \\\n",
       "0  Amazon - Echo Show is the best of all my Amazo...             1.0   \n",
       "1  This tablet is a steal for the price. bought t...             1.0   \n",
       "2  Everyone should have this product. It is great...             1.0   \n",
       "3  Great tablet. Drop it many times and its still...             1.0   \n",
       "4  I am normally a real book person, but I have a...             1.0   \n",
       "5  I bought this as a gift to my dad and he loves...             1.0   \n",
       "6  I bought this for my daughter. It is our 4th K...             1.0   \n",
       "7  I bought this for my kids its a cheap alternat...             1.0   \n",
       "8  I bought this with paper white and did not see...             1.0   \n",
       "9  I find myself saying to my phone, \"Alexa, play...             1.0   \n",
       "\n",
       "   positive  negative  neutral  admiration  angry  \\\n",
       "0       1.0       0.0      0.0         1.0    0.0   \n",
       "1       1.0       0.0      0.0         0.0    0.0   \n",
       "2       1.0       0.0      0.0         0.0    0.0   \n",
       "3       1.0       0.0      0.0         0.0    0.0   \n",
       "4       1.0       0.0      0.0         0.0    0.0   \n",
       "5       1.0       0.0      0.0         0.0    0.0   \n",
       "6       1.0       0.0      0.0         0.0    0.0   \n",
       "7       1.0       0.0      0.0         0.0    0.0   \n",
       "8       1.0       0.0      0.0         0.0    0.0   \n",
       "9       1.0       0.0      0.0         0.0    0.0   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0  amazon echo show be best amazon product i love...      0       0  \n",
       "1  tablet be steal price bought one sister read b...      0       0  \n",
       "2  everyone have product be great have be need se...      0       0  \n",
       "3            great tablet drop many time still work       0       0  \n",
       "4  i be normally real book person i have year old...      0       0  \n",
       "5  i bought gift dad love well dont plan reading ...      0       0  \n",
       "6  i bought daughter be th kindle bought family t...      0       0  \n",
       "7  i bought kid cheap alternative ipad mini lose ...      0       0  \n",
       "8  i bought paper white do not see much differenc...      0       1  \n",
       "9   i find say phone alexa play i wish make car too       0       0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "# df = df.drop(['reviews.numHelpful','Unnamed: 10'],axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n"
     ]
    }
   ],
   "source": [
    "def convtodec(x):\n",
    "    val = 128\n",
    "    ret = 0\n",
    "    for y in x:\n",
    "        if(y):\n",
    "            ret+=val\n",
    "        val=val>>1\n",
    "    return ret\n",
    "labelpowerset = []\n",
    "for row in df.iterrows():\n",
    "    tmp = row[1][3:9].tolist()\n",
    "    labelpowerset.append(convtodec(tmp))\n",
    "print(len(labelpowerset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "cntpowerset = {}\n",
    "for val in labelpowerset:\n",
    "    if(val in cntpowerset):\n",
    "        cntpowerset[val]+=1\n",
    "    else:\n",
    "        cntpowerset[val]=1\n",
    "\n",
    "cntpowersetlist = []\n",
    "for key,val in cntpowerset.items():\n",
    "    cntpowersetlist.append((val,key))\n",
    "cntpowersetlist.sort(reverse=True)\n",
    "print(len(cntpowersetlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {}\n",
    "revlabel = {}\n",
    "cnt = 0\n",
    "for val in cntpowersetlist:\n",
    "    label[val[1]] = cnt\n",
    "    revlabel[cnt] = val[1]\n",
    "    cnt+=1\n",
    "powset = [label[x] for x in labelpowerset]\n",
    "df['powerset'] = powset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{192: 0,\n",
       " 144: 1,\n",
       " 200: 2,\n",
       " 164: 3,\n",
       " 64: 4,\n",
       " 160: 5,\n",
       " 72: 6,\n",
       " 16: 7,\n",
       " 36: 8,\n",
       " 32: 9,\n",
       " 208: 10,\n",
       " 120: 11,\n",
       " 252: 12,\n",
       " 148: 13,\n",
       " 176: 14,\n",
       " 80: 15,\n",
       " 68: 16,\n",
       " 48: 17,\n",
       " 20: 18}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1207\n",
       "1      794\n",
       "2      732\n",
       "3      656\n",
       "4      279\n",
       "5      242\n",
       "6      192\n",
       "7      138\n",
       "8       97\n",
       "9       72\n",
       "10       5\n",
       "11       3\n",
       "13       2\n",
       "12       2\n",
       "14       1\n",
       "15       1\n",
       "17       1\n",
       "18       1\n",
       "16       1\n",
       "Name: powerset, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['powerset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n"
     ]
    }
   ],
   "source": [
    "df_tmp = df[df['powerset']<=20]\n",
    "print(len(df_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tmp[['Analysed','qmark','exmark']]\n",
    "Y = df_tmp['powerset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4070\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df_tmp['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2bin(xx):\n",
    "    ret = []\n",
    "    for i in range(8):\n",
    "        if(xx & (1<<i)):\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "    #print(xx)\n",
    "    ret.reverse()\n",
    "    #print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-54ce666006c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mcl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                         \u001b[0mtmp_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_senticnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                     \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconvtodec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msenticnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 8"
     ]
    }
   ],
   "source": [
    "col_names = ['Sarcastic','Joy','Sadness','Neutral','Admiration','Anger']\n",
    "# col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    clf = RandomForestClassifier(n_estimators=300)\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].tolist(),Y.iloc[test_index].tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(label[convtodec(tmp_list)])\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(label[convtodec(tmp_list2)])\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score_list = []\n",
    "    predict_score_list = []\n",
    "    for i in range(len(y_test)):\n",
    "        score_list.append(convert2bin(revlabel[y_test[i]]))\n",
    "        predict_score_list.append(convert2bin(revlabel[y_pred[i]]))\n",
    "    np_score_list = np.array(score_list)\n",
    "    transpose = np_score_list.T\n",
    "    score_list = transpose.tolist()\n",
    "\n",
    "    np_predict_score_list = np.array(predict_score_list)\n",
    "    transpose = np_predict_score_list.T\n",
    "    predict_score_list = transpose.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0] 8\n"
     ]
    }
   ],
   "source": [
    "mmm = convtodec(tmp_list)\n",
    "print(tmp_list,mmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{192: 0,\n",
       " 144: 1,\n",
       " 200: 2,\n",
       " 164: 3,\n",
       " 64: 4,\n",
       " 160: 5,\n",
       " 72: 6,\n",
       " 16: 7,\n",
       " 36: 8,\n",
       " 32: 9,\n",
       " 208: 10,\n",
       " 120: 11,\n",
       " 252: 12,\n",
       " 148: 13,\n",
       " 176: 14,\n",
       " 80: 15,\n",
       " 68: 16,\n",
       " 48: 17,\n",
       " 20: 18}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.144764</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.889065</td>\n",
       "      <td>0.915056</td>\n",
       "      <td>0.855236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.149155</td>\n",
       "      <td>0.327027</td>\n",
       "      <td>0.888043</td>\n",
       "      <td>0.912618</td>\n",
       "      <td>0.850845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.154054</td>\n",
       "      <td>0.302703</td>\n",
       "      <td>0.886569</td>\n",
       "      <td>0.909055</td>\n",
       "      <td>0.845946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.150845</td>\n",
       "      <td>0.312162</td>\n",
       "      <td>0.886960</td>\n",
       "      <td>0.911224</td>\n",
       "      <td>0.849155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.155068</td>\n",
       "      <td>0.309459</td>\n",
       "      <td>0.882994</td>\n",
       "      <td>0.908947</td>\n",
       "      <td>0.844932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.153078</td>\n",
       "      <td>0.307172</td>\n",
       "      <td>0.887323</td>\n",
       "      <td>0.910067</td>\n",
       "      <td>0.846922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.148512</td>\n",
       "      <td>0.300406</td>\n",
       "      <td>0.887771</td>\n",
       "      <td>0.912288</td>\n",
       "      <td>0.851488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.154263</td>\n",
       "      <td>0.317997</td>\n",
       "      <td>0.884679</td>\n",
       "      <td>0.909470</td>\n",
       "      <td>0.845737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.146820</td>\n",
       "      <td>0.335589</td>\n",
       "      <td>0.888271</td>\n",
       "      <td>0.913529</td>\n",
       "      <td>0.853180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.148512</td>\n",
       "      <td>0.315291</td>\n",
       "      <td>0.887497</td>\n",
       "      <td>0.912637</td>\n",
       "      <td>0.851488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.150507</td>\n",
       "      <td>0.316564</td>\n",
       "      <td>0.886917</td>\n",
       "      <td>0.911489</td>\n",
       "      <td>0.849493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.144764         0.337838       0.889065       0.915056   \n",
       "1         2      0.149155         0.327027       0.888043       0.912618   \n",
       "2         3      0.154054         0.302703       0.886569       0.909055   \n",
       "3         4      0.150845         0.312162       0.886960       0.911224   \n",
       "4         5      0.155068         0.309459       0.882994       0.908947   \n",
       "5         6      0.153078         0.307172       0.887323       0.910067   \n",
       "6         7      0.148512         0.300406       0.887771       0.912288   \n",
       "7         8      0.154263         0.317997       0.884679       0.909470   \n",
       "8         9      0.146820         0.335589       0.888271       0.913529   \n",
       "9        10      0.148512         0.315291       0.887497       0.912637   \n",
       "10  average      0.150507         0.316564       0.886917       0.911489   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.855236  \n",
       "1           0.850845  \n",
       "2           0.845946  \n",
       "3           0.849155  \n",
       "4           0.844932  \n",
       "5           0.846922  \n",
       "6           0.851488  \n",
       "7           0.845737  \n",
       "8           0.853180  \n",
       "9           0.851488  \n",
       "10          0.849493  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(8291, 9850) (740, 9850)\n",
      "(8291,) (740,)\n",
      "Hamming Loss: 0.1464527027027027\n",
      "Exact Prediction: 0.327027027027027\n",
      "Macro F-Score: 0.8798959172213772\n",
      "Micro F-Score: 0.9139112302651176\n",
      "Average Accuracy: 0.8535472972972973\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(8281, 9850) (740, 9850)\n",
      "(8281,) (740,)\n",
      "Hamming Loss: 0.15371621621621623\n",
      "Exact Prediction: 0.3324324324324324\n",
      "Macro F-Score: 0.877362330176236\n",
      "Micro F-Score: 0.9097222222222222\n",
      "Average Accuracy: 0.8462837837837839\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(8291, 9850) (740, 9850)\n",
      "(8291,) (740,)\n",
      "Hamming Loss: 0.15675675675675677\n",
      "Exact Prediction: 0.29324324324324325\n",
      "Macro F-Score: 0.8772920302613761\n",
      "Micro F-Score: 0.9074591144794576\n",
      "Average Accuracy: 0.8432432432432432\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(8317, 9850) (740, 9850)\n",
      "(8317,) (740,)\n",
      "Hamming Loss: 0.14560810810810812\n",
      "Exact Prediction: 0.3310810810810811\n",
      "Macro F-Score: 0.8810545159382425\n",
      "Micro F-Score: 0.9141776184786938\n",
      "Average Accuracy: 0.8543918918918919\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(8294, 9850) (740, 9850)\n",
      "(8294,) (740,)\n",
      "Hamming Loss: 0.15287162162162163\n",
      "Exact Prediction: 0.31351351351351353\n",
      "Macro F-Score: 0.878563603215996\n",
      "Micro F-Score: 0.9102449667757612\n",
      "Average Accuracy: 0.8471283783783783\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(8327, 9850) (739, 9850)\n",
      "(8327,) (739,)\n",
      "Hamming Loss: 0.15493910690121787\n",
      "Exact Prediction: 0.31799729364005414\n",
      "Macro F-Score: 0.8769340200400996\n",
      "Micro F-Score: 0.9087649402390439\n",
      "Average Accuracy: 0.8450608930987822\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(8284, 9850) (739, 9850)\n",
      "(8284,) (739,)\n",
      "Hamming Loss: 0.1564614343707713\n",
      "Exact Prediction: 0.2814614343707713\n",
      "Macro F-Score: 0.8758323087060024\n",
      "Micro F-Score: 0.9076569831286811\n",
      "Average Accuracy: 0.8435385656292286\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(8271, 9850) (739, 9850)\n",
      "(8271,) (739,)\n",
      "Hamming Loss: 0.1507104194857916\n",
      "Exact Prediction: 0.34370771312584575\n",
      "Macro F-Score: 0.8806145776457668\n",
      "Micro F-Score: 0.9115105770185719\n",
      "Average Accuracy: 0.8492895805142084\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(8293, 9850) (739, 9850)\n",
      "(8293,) (739,)\n",
      "Hamming Loss: 0.14817320703653586\n",
      "Exact Prediction: 0.3301759133964817\n",
      "Macro F-Score: 0.8812421999601198\n",
      "Micro F-Score: 0.9127142287764051\n",
      "Average Accuracy: 0.8518267929634642\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(8332, 9850) (739, 9850)\n",
      "(8332,) (739,)\n",
      "Hamming Loss: 0.1510487144790257\n",
      "Exact Prediction: 0.3342354533152909\n",
      "Macro F-Score: 0.8764552669772318\n",
      "Micro F-Score: 0.9110114598903838\n",
      "Average Accuracy: 0.8489512855209743\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.15167382876787477\n",
      "Average Subset Accuracy: 0.320487510514574\n",
      "Average Macro F-score: 0.8785246770142449\n",
      "Average Micro F-score: 0.9107173341274338\n",
      "Average of Average Accuracy: 0.8483261712321252\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    clf = SVC()\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].tolist(),Y.iloc[test_index].tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(label[convtodec(tmp_list)])\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(t,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(label[convtodec(tmp_list2)])\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score_list = []\n",
    "    predict_score_list = []\n",
    "    for i in range(len(y_test)):\n",
    "        score_list.append(convert2bin(revlabel[y_test[i]]))\n",
    "        predict_score_list.append(convert2bin(revlabel[y_pred[i]]))\n",
    "    np_score_list = np.array(score_list)\n",
    "    transpose = np_score_list.T\n",
    "    score_list = transpose.tolist()\n",
    "\n",
    "    np_predict_score_list = np.array(predict_score_list)\n",
    "    transpose = np_predict_score_list.T\n",
    "    predict_score_list = transpose.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.146453</td>\n",
       "      <td>0.327027</td>\n",
       "      <td>0.879896</td>\n",
       "      <td>0.913911</td>\n",
       "      <td>0.853547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.153716</td>\n",
       "      <td>0.332432</td>\n",
       "      <td>0.877362</td>\n",
       "      <td>0.909722</td>\n",
       "      <td>0.846284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.156757</td>\n",
       "      <td>0.293243</td>\n",
       "      <td>0.877292</td>\n",
       "      <td>0.907459</td>\n",
       "      <td>0.843243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.145608</td>\n",
       "      <td>0.331081</td>\n",
       "      <td>0.881055</td>\n",
       "      <td>0.914178</td>\n",
       "      <td>0.854392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.152872</td>\n",
       "      <td>0.313514</td>\n",
       "      <td>0.878564</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.847128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.317997</td>\n",
       "      <td>0.876934</td>\n",
       "      <td>0.908765</td>\n",
       "      <td>0.845061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.156461</td>\n",
       "      <td>0.281461</td>\n",
       "      <td>0.875832</td>\n",
       "      <td>0.907657</td>\n",
       "      <td>0.843539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.150710</td>\n",
       "      <td>0.343708</td>\n",
       "      <td>0.880615</td>\n",
       "      <td>0.911511</td>\n",
       "      <td>0.849290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.148173</td>\n",
       "      <td>0.330176</td>\n",
       "      <td>0.881242</td>\n",
       "      <td>0.912714</td>\n",
       "      <td>0.851827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.151049</td>\n",
       "      <td>0.334235</td>\n",
       "      <td>0.876455</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.848951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.151674</td>\n",
       "      <td>0.320488</td>\n",
       "      <td>0.878525</td>\n",
       "      <td>0.910717</td>\n",
       "      <td>0.848326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.146453         0.327027       0.879896       0.913911   \n",
       "1         2      0.153716         0.332432       0.877362       0.909722   \n",
       "2         3      0.156757         0.293243       0.877292       0.907459   \n",
       "3         4      0.145608         0.331081       0.881055       0.914178   \n",
       "4         5      0.152872         0.313514       0.878564       0.910245   \n",
       "5         6      0.154939         0.317997       0.876934       0.908765   \n",
       "6         7      0.156461         0.281461       0.875832       0.907657   \n",
       "7         8      0.150710         0.343708       0.880615       0.911511   \n",
       "8         9      0.148173         0.330176       0.881242       0.912714   \n",
       "9        10      0.151049         0.334235       0.876455       0.911011   \n",
       "10  average      0.151674         0.320488       0.878525       0.910717   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.853547  \n",
       "1           0.846284  \n",
       "2           0.843243  \n",
       "3           0.854392  \n",
       "4           0.847128  \n",
       "5           0.845061  \n",
       "6           0.843539  \n",
       "7           0.849290  \n",
       "8           0.851827  \n",
       "9           0.848951  \n",
       "10          0.848326  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
