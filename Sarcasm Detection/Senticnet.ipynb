{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "senticnet['very'][4] = \"#joy\"\n",
    "senticnet['very'][5] = \"#interest\"\n",
    "senticnet['very'][7] = '0.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17100</th>\n",
       "      <td>illusionist</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12638</th>\n",
       "      <td>famous</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25858</th>\n",
       "      <td>pederasty</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11645</th>\n",
       "      <td>epidemiologist</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28369</th>\n",
       "      <td>purulent</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#fear</td>\n",
       "      <td>-0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>approximative</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38229</th>\n",
       "      <td>vender</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31458</th>\n",
       "      <td>self-promoting</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7080</th>\n",
       "      <td>console</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22986</th>\n",
       "      <td>mundaneness</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word   Primary    Secondary Polarity\n",
       "17100     illusionist  #sadness     #disgust    -0.83\n",
       "12638          famous      #joy    #interest     0.77\n",
       "25858       pederasty  #sadness       #anger    -0.85\n",
       "11645  epidemiologist      #joy    #surprise    0.824\n",
       "28369        purulent  #sadness        #fear    -0.73\n",
       "1811    approximative  #sadness       #anger    -0.73\n",
       "38229          vender      #joy  #admiration    0.676\n",
       "31458  self-promoting  #sadness     #disgust    -0.98\n",
       "7080          console      #joy    #interest     0.35\n",
       "22986     mundaneness      #joy    #surprise    0.718"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from senticnet5 import senticnet\n",
    "import pandas as pd\n",
    "\n",
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "\n",
    "rem = ['not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "       \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "       \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'don', \"don't\", \n",
    "       'just', 'too', 'very', 'no', 'nor', 'only', 'own', 'same', 'again', 'against', 'but',]\n",
    "for s in rem:\n",
    "  stopwords.remove(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from textblob.taggers import NLTKTagger\n",
    "nltk_tagger = NLTKTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def Lemmetize(sen):\n",
    "    tokens = word_tokenize(sen)\n",
    "#     print(tokens)\n",
    "    \n",
    "#     lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "#     negatives = []\n",
    "#     NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "#     NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "#     CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "\n",
    "#     mark_neg = {}\n",
    "#     nflag = False\n",
    "#     for t in lem:\n",
    "#         if not t in stopwords:\n",
    "#             if t in CONJUCTION_WORDS:\n",
    "#                 nflag=False\n",
    "#             if(nflag==True):\n",
    "#                 mark_neg[t]=1\n",
    "#                 negatives.append(t)\n",
    "#             if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "#                 nflag=True \n",
    "# #     print(negatives)\n",
    "# #     print('\\n')\n",
    "# #     print(mark_neg)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,'n') for t in tokens]\n",
    "# #     print(lem)\n",
    "    return lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magazine\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(w):\n",
    "    w = re.sub('[^a-zA-Z]', ' ', str(w))\n",
    "    w = w.lower()\n",
    "    w = tokenize(w)\n",
    "    return w)\n",
    "#     s = []\n",
    "#     tblobp = []\n",
    "#     tblobs = []\n",
    "# #     print(w)\n",
    "#     for i in w:\n",
    "#         if not i in stopwords:\n",
    "#             c = TextBlob(i)\n",
    "#             pol = c.sentiment.polarity\n",
    "#             sub = c.sentiment.subjectivity\n",
    "#             tblobp.append(pol)\n",
    "#             tblobs.append(sub)\n",
    "#             if i in senticnet:\n",
    "#                 c = float(senticnet[i][7])\n",
    "#                 s.append(c)\n",
    "#             else:\n",
    "#                 i = wordnet_lemmatizer.lemmatize(i,'v')\n",
    "#                 if i in senticnet:\n",
    "#                     c = float(senticnet[i][7])\n",
    "#                     s.append(c)\n",
    "# #             print(i,senticnet[i][4],senticnet[i][5],senticnet[i][7])\n",
    "# #             print('\\n')\n",
    "#     sent = sum(s)*100/(len(s))\n",
    "#     tblobpol = sum(tblobp)*100/(len(tblobp))\n",
    "#     tblobsub = sum(tblobs)*100/(len(tblobs))\n",
    "#     return (sent,tblobpol,tblobsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8846153846153837, 2.6923076923076925, 14.1025641025641)\n"
     ]
    }
   ],
   "source": [
    "p ='None of first eight batteries I opened worked. Tested other batteries to confirm was a battery problem. Confirmed!!!Wow.'\n",
    "print(clean_token(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34.08571428571429, 15.185185185185183, 17.592592592592595)\n"
     ]
    }
   ],
   "source": [
    "p ='the good news is I now am the proud owner of 48 slightly oversized batteries that do not fit in most of my battery operated devices. Perfect! Out of 9400 reviews nobody noticed the size issue with these'\n",
    "print(clean_senti(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'tablet', 'is', 'a', 'steal', 'for', 'the', 'price', '.', 'bought', 'this', 'one', 'for', 'my', 'sister', '.', 'You', 'can', 'read', 'book', ',', 'magazine', 'Honestly', 'considering', 'buying', 'one', 'myself', 'a', 'new', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "print(Lemmetize(\"This tablet is a steal for the price. bought this one for my sister. You can read books, magazines Honestly considering buying one myself a new one.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_emo.to_csv(\"Senti.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.78, 13.333333333333334, 16.666666666666668)\n"
     ]
    }
   ],
   "source": [
    "print(clean_senti(\"I was depressed. He asked me to be happy. I am not depressed anymore.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"AMAZON MODIFIED.csv\",encoding='unicode_escape')\n",
    "\n",
    "# # Taking only useful data from dataset\n",
    "# dataset = data.iloc[:,[3]].values\n",
    "data = pd.DataFrame(dataset)\n",
    "data = data.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentpol = []\n",
    "tblobpol = []\n",
    "tblobsub = []\n",
    "c =0\n",
    "for review in dataset:\n",
    "#     print(review)\n",
    "#     print(str(review))\n",
    "    sentp,tblobp,tblobs = clean_senti(review)\n",
    "    sentpol.append(sentp)\n",
    "    tblobpol.append(tblobp)\n",
    "    tblobsub.append(tblobs)\n",
    "#     if c is 10:\n",
    "#         break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.DataFrame(data,columns=['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sent_Polarity</th>\n",
       "      <th>Tblob_Polarity</th>\n",
       "      <th>Tblob_Sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon - Echo Show is the best of all my Amazo...</td>\n",
       "      <td>28.825000</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>16.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tablet is a steal for the price. bought t...</td>\n",
       "      <td>-12.793333</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>9.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everyone should have this product. It is great...</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>10.454545</td>\n",
       "      <td>11.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great tablet. Drop it many times and its still...</td>\n",
       "      <td>28.128571</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>17.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am normally a real book person, but I have a...</td>\n",
       "      <td>3.579167</td>\n",
       "      <td>4.354839</td>\n",
       "      <td>16.827957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Sent_Polarity  \\\n",
       "0  Amazon - Echo Show is the best of all my Amazo...      28.825000   \n",
       "1  This tablet is a steal for the price. bought t...     -12.793333   \n",
       "2  Everyone should have this product. It is great...      23.766667   \n",
       "3  Great tablet. Drop it many times and its still...      28.128571   \n",
       "4  I am normally a real book person, but I have a...       3.579167   \n",
       "\n",
       "   Tblob_Polarity  Tblob_Sub  \n",
       "0       17.142857  16.607143  \n",
       "1        4.909091   9.030303  \n",
       "2       10.454545  11.818182  \n",
       "3       18.571429  17.857143  \n",
       "4        4.354839  16.827957  "
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sent_Polarity\"] = sentpol\n",
    "data[\"Tblob_Polarity\"] = tblobpol\n",
    "data[\"Tblob_Sub\"] = tblobsub\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"mix_polarity.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-12.793333333333335, 4.909090909090908, 9.03030303030303)\n"
     ]
    }
   ],
   "source": [
    "print(clean_senti(\"This tablet is a steal for the price. bought this one for my sister. You can read books, magazines Honestly considering buying one myself a new one.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(porter_stemmer.stem('this'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oversized', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "lem = wordnet_lemmatizer.lemmatize('DEPRESS','v')\n",
    "le = pos_tag(['oversized'])\n",
    "print(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought\n"
     ]
    }
   ],
   "source": [
    "print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
